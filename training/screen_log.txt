Loaded </home/sbini_loc/command_interaction/training/settings/MTL_conf.py> as configuration file.
CUDA acceleration available on <5> devices: <[0, 1, 2, 3, 4]>
Global seed set to 2
The TensorBoard logggers will be saved in <lightning_logs/MTL/eng/MSI/HS_msi>.
DATALOADER SELECT HOW TO LOAD DATASET
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]

   | Name         | Type        | Params
----------------------------------------------
0  | conv0        | Conv2d      | 810   
1  | pool         | AvgPool2d   | 0     
2  | bn1          | BatchNorm2d | 0     
3  | conv1        | Conv2d      | 72.9 K
4  | bn2          | BatchNorm2d | 0     
5  | conv2        | Conv2d      | 72.9 K
6  | bn3          | BatchNorm2d | 0     
7  | conv3        | Conv2d      | 72.9 K
8  | bn4          | BatchNorm2d | 0     
9  | conv4        | Conv2d      | 72.9 K
10 | bn5          | BatchNorm2d | 0     
11 | conv5        | Conv2d      | 72.9 K
12 | bn6          | BatchNorm2d | 0     
13 | conv6        | Conv2d      | 72.9 K
14 | out_intent   | Linear      | 2.1 K 
15 | out_explicit | Linear      | 5.6 K 
16 | out_implicit | Linear      | 182   
17 | loss_fn      | MT_loss     | 0     
----------------------------------------------
446 K     Trainable params
0         Non-trainable params
446 K     Total params
1.785     Total estimated model params size (MB)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] ../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.
Traceback (most recent call last):
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 559, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 935, in _run
    results = self._run_stage()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 978, in _run_stage
    self.fit_loop.run()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 261, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 142, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/core/module.py", line 1266, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py", line 158, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 224, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/optim/adam.py", line 121, in step
    loss = closure()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 135, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 233, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 288, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 199, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 67, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/sbini_loc/command_interaction/training/models/MSI_hardsharing.py", line 311, in backward
    loss.backward(retain_graph=self.grad_norm)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sbini_loc/command_interaction/training/train.py", line 202, in <module>
    trainer.fit(model=model, train_dataloaders=model.train_loader, val_dataloaders=model.val_loader)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 520, in fit
    call._call_and_handle_interrupt(
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 958, in _teardown
    self.strategy.teardown()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 475, in teardown
    self.lightning_module.cpu()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 78, in cpu
    return super().cpu()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Epoch 0:   0%|          | 0/2 [00:11<?, ?it/s]
