Loaded </home/sbini_loc/command_interaction/training/settings/MTL_conf.py> as configuration file.
CUDA acceleration available on <5> devices: <[0, 1, 2, 3, 4]>
Global seed set to 2
The TensorBoard logggers will be saved in <lightning_logs/MTL/eng/MSI/HS_msi>.
DATALOADER SELECT HOW TO LOAD DATASET
Traceback (most recent call last):
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 260, in _lazy_init
    queued_call()
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 145, in _check_capability
    capability = get_device_capability(d)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 381, in get_device_capability
    prop = get_device_properties(device)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 399, in get_device_properties
    return _get_device_properties(device)  # type: ignore[name-defined]
RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at "../aten/src/ATen/cuda/CUDAContext.cpp":50, please report a bug to PyTorch. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/sbini_loc/command_interaction/training/train.py", line 164, in <module>
    model = HardSharing_MSI(settings=settings, task_n_labels=task_n_labels, task_loss_weights=np.array(object=(None, None, None))).cuda(settings.training.device)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 73, in cuda
    return super().cuda(device=device)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 905, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 905, in <lambda>
    return self._apply(lambda t: t.cuda(device))
  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 264, in _lazy_init
    raise DeferredCudaCallError(msg) from e
torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at "../aten/src/ATen/cuda/CUDAContext.cpp":50, please report a bug to PyTorch. 

CUDA call was originally invoked at:

['  File "/home/sbini_loc/command_interaction/training/train.py", line 13, in <module>\n    import torch\n', '  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load\n', '  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked\n', '  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked\n', '  File "<frozen importlib._bootstrap_external>", line 850, in exec_module\n', '  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed\n', '  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/__init__.py", line 1146, in <module>\n    _C._initExtension(manager_path())\n', '  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load\n', '  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked\n', '  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked\n', '  File "<frozen importlib._bootstrap_external>", line 850, in exec_module\n', '  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed\n', '  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 197, in <module>\n    _lazy_call(_check_capability)\n', '  File "/home/sbini_loc/anaconda3/envs/dgx2/lib/python3.9/site-packages/torch/cuda/__init__.py", line 195, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n']
