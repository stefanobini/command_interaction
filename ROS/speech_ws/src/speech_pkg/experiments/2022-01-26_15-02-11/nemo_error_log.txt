[NeMo W 2022-01-26 14:51:59 optimizers:52] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-01-26 14:52:05 experimental:27] Module <function get_argmin_mat at 0x7f6e1b1c3c10> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-26 14:52:05 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f6e246719d0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-26 14:52:05 experimental:27] Module <function parse_scale_configs at 0x7f6e1b1c94c0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-26 14:52:05 experimental:27] Module <function get_embs_and_timestamps at 0x7f6e1b1c9550> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-26 15:02:11 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:48: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-26 15:02:11 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-01-26 15:02:11 exp_manager:889] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-01-26 15:02:11 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-26 15:02:12 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-26 15:02:12 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-26 15:02:36 modelPT:475] The lightning trainer received accelerator: <pytorch_lightning.accelerators.gpu.GPUAccelerator object at 0x7f6e0c4ad070>. We recommend to use 'ddp' instead.
[NeMo W 2022-01-26 15:03:03 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:485: UserWarning: One of given dataloaders is None and it will be skipped.
      rank_zero_warn("One of given dataloaders is None and it will be skipped.")
    
[NeMo W 2022-01-26 15:03:03 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
