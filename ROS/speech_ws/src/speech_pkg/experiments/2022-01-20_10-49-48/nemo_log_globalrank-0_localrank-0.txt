[NeMo W 2022-01-31 14:12:59 optimizers:52] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-01-31 14:13:01 experimental:27] Module <function get_argmin_mat at 0x7f782eacdc10> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-31 14:13:01 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f78386599d0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-31 14:13:01 experimental:27] Module <function parse_scale_configs at 0x7f782ead34c0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-31 14:13:01 experimental:27] Module <function get_embs_and_timestamps at 0x7f782ead3550> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 1310 files loaded accounting to # 29 labels
[NeMo W 2022-01-31 14:22:56 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:48: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-31 14:22:56 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo I 2022-01-31 14:22:56 collections:266] Filtered duration for loading collection is 0.000000.
[NeMo I 2022-01-31 14:22:56 collections:270] # 6054 files loaded accounting to # 29 labels
[NeMo I 2022-01-31 14:22:56 exp_manager:283] Experiments will be logged at /user/gdesimone/tesi_-magistrale/nemo_experiments/MatchboxNet-3x2x64/2022-01-31_14-22-56
[NeMo I 2022-01-31 14:22:56 exp_manager:648] TensorboardLogger has been set up
[NeMo W 2022-01-31 14:22:56 exp_manager:889] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-01-31 14:22:56 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-31 14:22:57 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-31 14:22:57 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-31 14:23:00 modelPT:475] The lightning trainer received accelerator: <pytorch_lightning.accelerators.gpu.GPUAccelerator object at 0x7f781ddf18b0>. We recommend to use 'ddp' instead.
[NeMo I 2022-01-31 14:23:00 modelPT:566] Optimizer config = Novograd (
    Parameter Group 0
        amsgrad: False
        betas: [0.95, 0.5]
        eps: 1e-08
        grad_averaging: False
        lr: 0.05
        weight_decay: 0.001
    )
[NeMo I 2022-01-31 14:23:00 lr_scheduler:833] Scheduler "<nemo.core.optim.lr_scheduler.PolynomialHoldDecayAnnealing object at 0x7f781b3f2490>" 
    will be used during training (effective maximum steps = 1500) - 
    Parameters : 
    (power: 2.0
    warmup_ratio: 0.05
    hold_ratio: 0.45
    min_lr: 0.001
    last_epoch: -1
    max_steps: 1500
    )
[NeMo W 2022-01-31 14:23:28 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:485: UserWarning: One of given dataloaders is None and it will be skipped.
      rank_zero_warn("One of given dataloaders is None and it will be skipped.")
    
[NeMo W 2022-01-31 14:23:28 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
