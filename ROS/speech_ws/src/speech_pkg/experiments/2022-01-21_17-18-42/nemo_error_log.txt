[NeMo W 2022-01-21 17:07:39 optimizers:52] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-01-21 17:07:41 experimental:27] Module <function get_argmin_mat at 0x7f6ac7937c10> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-21 17:07:41 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f6acc49d9d0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-21 17:07:41 experimental:27] Module <function parse_scale_configs at 0x7f6ac793d4c0> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-21 17:07:41 experimental:27] Module <function get_embs_and_timestamps at 0x7f6ac793d550> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-01-21 17:18:42 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:48: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-21 17:18:42 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-01-21 17:18:42 exp_manager:889] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-01-21 17:18:42 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-01-21 17:18:43 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-21 17:18:43 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
      rank_zero_deprecation(
    
[NeMo W 2022-01-21 17:18:46 modelPT:475] The lightning trainer received accelerator: <pytorch_lightning.accelerators.gpu.GPUAccelerator object at 0x7f6ac4a7b640>. We recommend to use 'ddp' instead.
[NeMo W 2022-01-21 17:19:14 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:485: UserWarning: One of given dataloaders is None and it will be skipped.
      rank_zero_warn("One of given dataloaders is None and it will be skipped.")
    
[NeMo W 2022-01-21 17:19:14 nemo_logging:349] /user/gdesimone/miniconda3/envs/tesi/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
